2. LLM（大規模言語モデル）とは
2.1 LLMとは何か？
**LLM（Large Language Model）**は、膨大なテキストデータを学習して自然言語の理解・生成ができるAIモデル。

ChatGPT、Claude、Geminiなどが代表例。

応用例：チャットボット、自動要約、検索、コード生成、翻訳など。

2.2 TransformerとAttentionの仕組み
2017年にGoogleが発表したTransformerアーキテクチャがベース。

Transformerの革新：

全文を一度に処理し、「文脈（文のどこが重要か）」を動的に重み付けする。

これを「Attention（注意機構）」と呼ぶ。

Attentionの基本式
Attention
(
𝑄
,
𝐾
,
𝑉
)
=
softmax
(
𝑄
𝐾
𝑇
𝑑
𝑘
)
𝑉
Attention(Q,K,V)=softmax( 
d 
k
​
 
​
 
QK 
T
 
​
 )V
$Q$（Query）：入力単語のベクトル

$K$（Key）：各単語の「意味」を表すベクトル

$V$（Value）：各単語の「値」

$\text{softmax}$：重要度（重み）を計算して合計1にする

意味：
それぞれの単語同士の関係を「ベクトルの類似度（内積）」で計算し、重要な単語ほど大きな重みを付けて情報を取り出す仕組み。

2.3 ベクトルと意味の関係（簡単な数学）
単語や文を「ベクトル（数値の並び）」で表現する（＝埋め込み）。

例：[0.1, 0.3, -0.2, ...]

ベクトルの内積や距離が「意味の近さ」「関連性」を表す。

類似した意味の単語 → ベクトルも近い

【例】
「猫」と「犬」のベクトルは近いが、「猫」と「車」は遠い。

2.4 ベクトル表現が重要な理由
検索・分類・要約・生成など、多くのAIタスクで「ベクトル空間上の操作」が本質的。

ベクトルの計算が高速＆大規模並列できるため、AIの高度な推論・応答が現実に。

2.5 応用と限界
事前知識が膨大でも、「知っていること」しか答えられない

最新の情報や個別の知識は得意ではない

ここが次章（RAG）につながる！